





# Importaci√≥n de librer√≠as est√°ndar
import os
import sys
import logging
from pathlib import Path
from datetime import datetime
import warnings

# Librer√≠as de manipulaci√≥n de datos
import pandas as pd
import numpy as np
import pyarrow.parquet as pq

# MongoDB
from pymongo import MongoClient, ASCENDING, DESCENDING
from pymongo.errors import ConnectionFailure, BulkWriteError

# Visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns

# Configuraci√≥n de estilo visual
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# Configuraci√≥n de pandas para mejor visualizaci√≥n
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.2f}'.format)

print("‚úÖ Librer√≠as importadas correctamente")
print(f"üì¶ Pandas version: {pd.__version__}")
print(f"üì¶ NumPy version: {np.__version__}")


# Configuraci√≥n de logging profesional
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./practica_mongodb.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

logger.info("=" * 80)
logger.info("INICIO DEL PROYECTO: An√°lisis HVFHV NYC - MongoDB")
logger.info("=" * 80)


# Creaci√≥n de estructura de carpetas (rutas relativas)
folders = ['./data', './scripts', './outputs', './report']

for folder in folders:
    Path(folder).mkdir(parents=True, exist_ok=True)
    logger.info(f"üìÅ Carpeta verificada/creada: {folder}")

print("\n‚úÖ Estructura de carpetas lista")


# Configuraci√≥n de conexi√≥n a MongoDB
# NOTA: Ajustar seg√∫n tu instalaci√≥n (local o Atlas)

MONGO_URI = "mongodb://localhost:27017/"  # Local
# MONGO_URI = "mongodb+srv://user:pass@cluster.mongodb.net/"  # Atlas

DATABASE_NAME = "nyc_hvfhv_db"
COLLECTION_NAME = "trips"

# Par√°metros de procesamiento
BATCH_SIZE = 20000  # Tama√±o de lotes para inserci√≥n
CHUNK_SIZE = 100000  # Tama√±o de chunks para lectura Parquet

logger.info(f"üîó MongoDB URI configurado: {MONGO_URI}")
logger.info(f"üíæ Base de datos: {DATABASE_NAME}")
logger.info(f"üì¶ Colecci√≥n: {COLLECTION_NAME}")





# Listar archivos Parquet disponibles
data_path = Path('./data')
parquet_files = sorted(data_path.glob('*.parquet'))

print(f"üìÇ Archivos Parquet encontrados: {len(parquet_files)}\n")
for idx, file in enumerate(parquet_files, 1):
    file_size_mb = file.stat().st_size / (1024 * 1024)
    print(f"  {idx}. {file.name} ({file_size_mb:.2f} MB)")
    
if not parquet_files:
    logger.warning("‚ö†Ô∏è  No se encontraron archivos Parquet en ./data/")
    logger.info("üí° Aseg√∫rate de haber descargado los archivos HVFHV 2025 (Q1-Q2)")
else:
    logger.info(f"‚úÖ {len(parquet_files)} archivos Parquet detectados")


# Funci√≥n de ingesta modular
def ingest_parquet_file(file_path, sample_rows=None):
    """
    Lee un archivo Parquet y retorna un DataFrame de pandas.
    
    Args:
        file_path (Path): Ruta al archivo Parquet
        sample_rows (int, optional): N√∫mero de filas a leer (None = todas)
    
    Returns:
        pd.DataFrame: DataFrame con los datos cargados
    """
    try:
        logger.info(f"üìñ Leyendo archivo: {file_path.name}")
        
        # Leer Parquet con PyArrow
        if sample_rows:
            df = pd.read_parquet(file_path, engine='pyarrow').head(sample_rows)
            logger.info(f"   ‚ö° Muestra de {sample_rows} filas cargadas")
        else:
            df = pd.read_parquet(file_path, engine='pyarrow')
            logger.info(f"   üìä Total de {len(df):,} filas cargadas")
        
        # Informaci√≥n de memoria
        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)
        logger.info(f"   üíæ Uso de memoria: {memory_mb:.2f} MB")
        
        return df
    
    except Exception as e:
        logger.error(f"‚ùå Error al leer {file_path.name}: {str(e)}")
        raise


# Cargar primer archivo para an√°lisis exploratorio
if parquet_files:
    sample_file = parquet_files[0]
    df_sample = ingest_parquet_file(sample_file, sample_rows=50000)
    
    print(f"\n{'='*80}")
    print(f"üìã AN√ÅLISIS DEL DATASET: {sample_file.name}")
    print(f"{'='*80}\n")
    
    print(f"üìê Dimensiones: {df_sample.shape[0]:,} filas √ó {df_sample.shape[1]} columnas\n")
    
    print("üìä Informaci√≥n del DataFrame:")
    print(df_sample.info())
else:
    print("‚ö†Ô∏è  No hay archivos para analizar. Carga los datos en ./data/")


# Primeras filas del dataset
if 'df_sample' in locals():
    print("\nüîç Primeras 5 filas del dataset:\n")
    display(df_sample.head())


# An√°lisis de valores nulos
if 'df_sample' in locals():
    null_counts = df_sample.isnull().sum()
    null_percentages = (null_counts / len(df_sample)) * 100
    
    null_df = pd.DataFrame({
        'Columna': null_counts.index,
        'Valores Nulos': null_counts.values,
        'Porcentaje (%)': null_percentages.values
    }).sort_values('Valores Nulos', ascending=False)
    
    print("\nüìä AN√ÅLISIS DE VALORES NULOS:\n")
    print(null_df[null_df['Valores Nulos'] > 0].to_string(index=False))
    
    if null_df['Valores Nulos'].sum() == 0:
        print("‚úÖ No se encontraron valores nulos en la muestra")


# Estad√≠sticas descriptivas de columnas num√©ricas
if 'df_sample' in locals():
    print("\nüìà ESTAD√çSTICAS DESCRIPTIVAS:\n")
    display(df_sample.describe())





# Funci√≥n de limpieza modular
def clean_hvfhv_data(df):
    """
    Limpia y transforma el DataFrame de viajes HVFHV.
    
    Args:
        df (pd.DataFrame): DataFrame original
    
    Returns:
        pd.DataFrame: DataFrame limpio
    """
    logger.info("üßπ Iniciando limpieza de datos...")
    initial_rows = len(df)
    
    # 1. Convertir timestamps a datetime
    datetime_cols = ['pickup_datetime', 'dropoff_datetime', 'request_datetime', 'on_scene_datetime']
    for col in datetime_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            logger.info(f"   ‚úì Convertido {col} a datetime")
    
    # 2. Calcular duraci√≥n del viaje en segundos y minutos
    if 'pickup_datetime' in df.columns and 'dropoff_datetime' in df.columns:
        df['trip_duration_seconds'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds()
        df['trip_duration_minutes'] = df['trip_duration_seconds'] / 60
        logger.info("   ‚úì Calculada duraci√≥n del viaje")
    
    # 3. Filtrar outliers de duraci√≥n (> 0 segundos y < 4 horas)
    if 'trip_duration_seconds' in df.columns:
        df = df[(df['trip_duration_seconds'] > 0) & (df['trip_duration_seconds'] < 14400)]
        logger.info(f"   ‚úì Filtrados viajes con duraci√≥n v√°lida (0s - 4h)")
    
    # 4. Filtrar distancias v√°lidas (si existe la columna)
    if 'trip_miles' in df.columns:
        df = df[(df['trip_miles'] >= 0) & (df['trip_miles'] < 200)]
        logger.info(f"   ‚úì Filtradas distancias v√°lidas (0 - 200 millas)")
    
    # 5. Eliminar nulos en campos cr√≠ticos
    critical_cols = ['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID']
    df = df.dropna(subset=[col for col in critical_cols if col in df.columns])
    logger.info(f"   ‚úì Eliminados nulos en campos cr√≠ticos")
    
    # 6. Feature Engineering: hora del d√≠a, d√≠a de la semana
    if 'pickup_datetime' in df.columns:
        df['pickup_hour'] = df['pickup_datetime'].dt.hour
        df['pickup_day_of_week'] = df['pickup_datetime'].dt.dayofweek  # 0=Lunes, 6=Domingo
        df['pickup_day_name'] = df['pickup_datetime'].dt.day_name()
        df['pickup_date'] = df['pickup_datetime'].dt.date
        logger.info(f"   ‚úì Features temporales creadas")
    
    # 7. Convertir IDs a enteros
    id_cols = ['PULocationID', 'DOLocationID', 'hvfhs_license_num']
    for col in id_cols:
        if col in df.columns:
            df[col] = df[col].astype(str)
    
    final_rows = len(df)
    removed_rows = initial_rows - final_rows
    logger.info(f"‚úÖ Limpieza completada: {removed_rows:,} filas eliminadas ({(removed_rows/initial_rows)*100:.2f}%)")
    logger.info(f"üìä Filas restantes: {final_rows:,}")
    
    return df


# Aplicar limpieza a la muestra
if 'df_sample' in locals():
    df_clean = clean_hvfhv_data(df_sample.copy())
    
    print(f"\n{'='*80}")
    print(f"üìä RESUMEN POST-LIMPIEZA")
    print(f"{'='*80}\n")
    print(f"üìê Nuevas dimensiones: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\n")
    
    print("üÜï Nuevas columnas creadas:")
    new_cols = ['trip_duration_seconds', 'trip_duration_minutes', 'pickup_hour', 
                'pickup_day_of_week', 'pickup_day_name', 'pickup_date']
    for col in new_cols:
        if col in df_clean.columns:
            print(f"   ‚úì {col}")
    
    print("\nüìã Primeras 5 filas del dataset limpio:\n")
    display(df_clean.head())





# Funci√≥n para conectar a MongoDB
def connect_to_mongodb(uri, db_name):
    """
    Establece conexi√≥n con MongoDB.
    
    Args:
        uri (str): URI de conexi√≥n a MongoDB
        db_name (str): Nombre de la base de datos
    
    Returns:
        tuple: (client, database)
    """
    try:
        logger.info("üîó Conectando a MongoDB...")
        client = MongoClient(uri, serverSelectionTimeoutMS=5000)
        
        # Verificar conexi√≥n
        client.admin.command('ping')
        logger.info("‚úÖ Conexi√≥n exitosa a MongoDB")
        
        db = client[db_name]
        logger.info(f"üíæ Base de datos seleccionada: {db_name}")
        
        return client, db
    
    except ConnectionFailure as e:
        logger.error(f"‚ùå Error de conexi√≥n a MongoDB: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"‚ùå Error inesperado: {str(e)}")
        raise


# Establecer conexi√≥n
try:
    mongo_client, mongo_db = connect_to_mongodb(MONGO_URI, DATABASE_NAME)
    collection = mongo_db[COLLECTION_NAME]
    
    print(f"‚úÖ Conectado a MongoDB")
    print(f"üìä Base de datos: {DATABASE_NAME}")
    print(f"üì¶ Colecci√≥n: {COLLECTION_NAME}")
    
    # Verificar colecciones existentes
    existing_collections = mongo_db.list_collection_names()
    print(f"\nüìã Colecciones existentes en la BD: {existing_collections}")
    
except Exception as e:
    print(f"‚ùå Error de conexi√≥n: {str(e)}")
    print("üí° Verifica que MongoDB est√© ejecut√°ndose y el URI sea correcto")


# Funci√≥n para insertar datos en MongoDB por lotes
def insert_data_to_mongodb(df, collection, batch_size=20000):
    """
    Inserta datos del DataFrame a MongoDB en lotes.
    
    Args:
        df (pd.DataFrame): DataFrame a insertar
        collection: Colecci√≥n de MongoDB
        batch_size (int): Tama√±o de cada lote
    
    Returns:
        int: N√∫mero total de documentos insertados
    """
    try:
        logger.info(f"üíæ Insertando {len(df):,} documentos en lotes de {batch_size:,}...")
        
        # Convertir DataFrame a lista de diccionarios
        # Convertir timestamps a formato compatible con MongoDB
        df_copy = df.copy()
        
        # Convertir datetime.date a datetime para MongoDB
        for col in df_copy.columns:
            if df_copy[col].dtype == 'object':
                # Verificar si contiene fechas
                if 'date' in col.lower():
                    try:
                        df_copy[col] = pd.to_datetime(df_copy[col])
                    except:
                        pass
        
        records = df_copy.to_dict('records')
        
        # Insertar en lotes
        total_inserted = 0
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            result = collection.insert_many(batch, ordered=False)
            total_inserted += len(result.inserted_ids)
            logger.info(f"   ‚úì Lote {i//batch_size + 1}: {len(result.inserted_ids):,} documentos insertados")
        
        logger.info(f"‚úÖ Total de documentos insertados: {total_inserted:,}")
        return total_inserted
    
    except BulkWriteError as bwe:
        logger.warning(f"‚ö†Ô∏è  Algunos documentos no se insertaron: {len(bwe.details['writeErrors'])} errores")
        return len(records) - len(bwe.details['writeErrors'])
    except Exception as e:
        logger.error(f"‚ùå Error al insertar datos: {str(e)}")
        raise


# OPCI√ìN 1: Insertar solo la muestra (para pruebas r√°pidas)
# Descomenta si solo quieres insertar la muestra

# if 'df_clean' in locals() and 'collection' in locals():
#     print("‚ö° Insertando muestra de datos...")
#     inserted_count = insert_data_to_mongodb(df_clean, collection, BATCH_SIZE)
#     print(f"‚úÖ {inserted_count:,} documentos insertados en la colecci√≥n '{COLLECTION_NAME}'")

print("üí° Celda lista para insertar muestra (descomenta para ejecutar)")


# OPCI√ìN 2: Cargar e insertar TODOS los archivos Parquet (6 meses completos)
# ‚ö†Ô∏è ADVERTENCIA: Esto puede tomar varios minutos y usar mucha memoria

def load_all_parquet_files(parquet_files, collection, batch_size=20000):
    """
    Carga todos los archivos Parquet y los inserta en MongoDB.
    
    Args:
        parquet_files (list): Lista de rutas a archivos Parquet
        collection: Colecci√≥n de MongoDB
        batch_size (int): Tama√±o de lotes para inserci√≥n
    
    Returns:
        int: Total de documentos insertados
    """
    total_inserted = 0
    
    for idx, file_path in enumerate(parquet_files, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"Procesando archivo {idx}/{len(parquet_files)}: {file_path.name}")
        logger.info(f"{'='*80}")
        
        try:
            # Leer archivo completo
            df = ingest_parquet_file(file_path, sample_rows=None)
            
            # Limpiar datos
            df_clean = clean_hvfhv_data(df)
            
            # Insertar en MongoDB
            inserted = insert_data_to_mongodb(df_clean, collection, batch_size)
            total_inserted += inserted
            
            logger.info(f"‚úÖ Archivo {file_path.name} completado: {inserted:,} documentos")
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando {file_path.name}: {str(e)}")
            continue
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üéâ CARGA COMPLETA: {total_inserted:,} documentos totales insertados")
    logger.info(f"{'='*80}")
    
    return total_inserted

# Descomenta para cargar todos los archivos
# if parquet_files and 'collection' in locals():
#     total_docs = load_all_parquet_files(parquet_files, collection, BATCH_SIZE)
#     print(f"\n‚úÖ Carga completa: {total_docs:,} documentos en MongoDB")

print("üí° Celda lista para cargar TODOS los archivos (descomenta para ejecutar)")


# Crear √≠ndices para optimizar consultas
def create_indexes(collection):
    """
    Crea √≠ndices en la colecci√≥n para optimizar consultas.
    
    Args:
        collection: Colecci√≥n de MongoDB
    """
    try:
        logger.info("üîß Creando √≠ndices...")
        
        # √çndices individuales
        indexes = [
            ('pickup_datetime', ASCENDING),
            ('dropoff_datetime', ASCENDING),
            ('PULocationID', ASCENDING),
            ('DOLocationID', ASCENDING),
            ('pickup_hour', ASCENDING),
            ('pickup_day_of_week', ASCENDING)
        ]
        
        for field, direction in indexes:
            collection.create_index([(field, direction)])
            logger.info(f"   ‚úì √çndice creado en: {field}")
        
        # √çndice compuesto para queries comunes
        collection.create_index([
            ('pickup_datetime', ASCENDING),
            ('PULocationID', ASCENDING)
        ])
        logger.info(f"   ‚úì √çndice compuesto creado: pickup_datetime + PULocationID")
        
        logger.info("‚úÖ Todos los √≠ndices creados exitosamente")
        
        # Mostrar √≠ndices creados
        indexes_list = collection.list_indexes()
        print("\nüìã √çndices en la colecci√≥n:")
        for idx in indexes_list:
            print(f"   ‚Ä¢ {idx['name']}")
    
    except Exception as e:
        logger.error(f"‚ùå Error creando √≠ndices: {str(e)}")
        raise

# Crear √≠ndices (ejecutar despu√©s de insertar datos)
if 'collection' in locals():
    create_indexes(collection)


# Verificar datos en MongoDB
if 'collection' in locals():
    doc_count = collection.count_documents({})
    print(f"\nüìä ESTADO DE LA COLECCI√ìN")
    print(f"{'='*80}")
    print(f"üì¶ Colecci√≥n: {COLLECTION_NAME}")
    print(f"üìÑ Total de documentos: {doc_count:,}")
    
    if doc_count > 0:
        # Mostrar un documento de ejemplo
        sample_doc = collection.find_one()
        print(f"\nüîç Documento de ejemplo:")
        print(f"{'='*80}")
        for key, value in list(sample_doc.items())[:10]:  # Primeros 10 campos
            print(f"  {key}: {value}")
        print("  ...")
    else:
        print("\n‚ö†Ô∏è  La colecci√≥n est√° vac√≠a. Ejecuta las celdas de inserci√≥n primero.")





# Verificar que tenemos datos antes de consultar
if 'collection' in locals():
    doc_count = collection.count_documents({})
    if doc_count == 0:
        print("‚ö†Ô∏è  ADVERTENCIA: La colecci√≥n est√° vac√≠a.")
        print("üí° Ejecuta primero las celdas de inserci√≥n de datos (Secci√≥n 4)")
    else:
        print(f"‚úÖ Colecci√≥n lista con {doc_count:,} documentos")
        print("üöÄ Procediendo con las consultas...\n")
else:
    print("‚ùå No hay conexi√≥n a MongoDB. Ejecuta la Secci√≥n 4 primero.")





# Aggregation Pipeline: Viajes por hora del d√≠a
if 'collection' in locals() and collection.count_documents({}) > 0:
    pipeline_trips_by_hour = [
        {
            "$group": {
                "_id": "$pickup_hour",
                "total_trips": {"$sum": 1},
                "avg_duration_min": {"$avg": "$trip_duration_minutes"}
            }
        },
        {
            "$sort": {"_id": 1}
        }
    ]
    
    logger.info("üîç Ejecutando consulta: Viajes por hora del d√≠a")
    results = list(collection.aggregate(pipeline_trips_by_hour))
    
    # Convertir a DataFrame para an√°lisis
    df_trips_by_hour = pd.DataFrame(results)
    df_trips_by_hour.columns = ['Hora', 'Total_Viajes', 'Duracion_Promedio_Min']
    
    print("\nüìä VIAJES POR HORA DEL D√çA")
    print("="*80)
    display(df_trips_by_hour)
    
    # Identificar hora pico
    hora_pico = df_trips_by_hour.loc[df_trips_by_hour['Total_Viajes'].idxmax()]
    print(f"\nüî• HORA PICO: {int(hora_pico['Hora'])}:00 hrs con {int(hora_pico['Total_Viajes']):,} viajes")
else:
    print("‚ö†Ô∏è  No hay datos para consultar")


# Visualizaci√≥n: Viajes por hora del d√≠a
if 'df_trips_by_hour' in locals():
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))
    
    # Gr√°fico 1: Total de viajes
    ax1.bar(df_trips_by_hour['Hora'], df_trips_by_hour['Total_Viajes'], 
            color='steelblue', edgecolor='black', alpha=0.7)
    ax1.set_xlabel('Hora del D√≠a', fontsize=12, fontweight='bold')
    ax1.set_ylabel('N√∫mero de Viajes', fontsize=12, fontweight='bold')
    ax1.set_title('üìä Distribuci√≥n de Viajes por Hora del D√≠a', fontsize=14, fontweight='bold')
    ax1.set_xticks(range(0, 24))
    ax1.grid(axis='y', alpha=0.3)
    ax1.ticklabel_format(style='plain', axis='y')
    
    # Gr√°fico 2: Duraci√≥n promedio
    ax2.plot(df_trips_by_hour['Hora'], df_trips_by_hour['Duracion_Promedio_Min'],
             marker='o', linewidth=2, markersize=8, color='coral')
    ax2.set_xlabel('Hora del D√≠a', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Duraci√≥n Promedio (minutos)', fontsize=12, fontweight='bold')
    ax2.set_title('‚è±Ô∏è Duraci√≥n Promedio de Viajes por Hora', fontsize=14, fontweight='bold')
    ax2.set_xticks(range(0, 24))
    ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    
    # Guardar gr√°fico
    output_path = Path('./outputs/trips_by_hour.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"üíæ Gr√°fico guardado en: {output_path}")
    
    plt.show()





# Aggregation Pipeline: Top 10 zonas de recogida
if 'collection' in locals() and collection.count_documents({}) > 0:
    pipeline_top_pickup = [
        {
            "$group": {
                "_id": "$PULocationID",
                "total_pickups": {"$sum": 1},
                "avg_trip_duration": {"$avg": "$trip_duration_minutes"}
            }
        },
        {
            "$sort": {"total_pickups": -1}
        },
        {
            "$limit": 10
        }
    ]
    
    logger.info("üîç Ejecutando consulta: Top 10 zonas de recogida")
    results = list(collection.aggregate(pipeline_top_pickup))
    
    df_top_pickup = pd.DataFrame(results)
    df_top_pickup.columns = ['LocationID', 'Total_Recogidas', 'Duracion_Promedio_Min']
    
    print("\nüìç TOP 10 ZONAS DE RECOGIDA M√ÅS ACTIVAS")
    print("="*80)
    display(df_top_pickup)
else:
    print("‚ö†Ô∏è  No hay datos para consultar")


# Visualizaci√≥n: Top 10 zonas de recogida
if 'df_top_pickup' in locals():
    fig, ax = plt.subplots(figsize=(12, 6))
    
    bars = ax.barh(df_top_pickup['LocationID'].astype(str), 
                    df_top_pickup['Total_Recogidas'],
                    color=sns.color_palette('viridis', len(df_top_pickup)),
                    edgecolor='black', alpha=0.8)
    
    ax.set_xlabel('N√∫mero de Recogidas', fontsize=12, fontweight='bold')
    ax.set_ylabel('Location ID', fontsize=12, fontweight='bold')
    ax.set_title('üìç Top 10 Zonas con Mayor Demanda de Recogidas', fontsize=14, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)
    
    # A√±adir valores en las barras
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2, 
                f'{int(width):,}',
                ha='left', va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    
    output_path = Path('./outputs/top_pickup_zones.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"üíæ Gr√°fico guardado en: {output_path}")
    
    plt.show()





# Aggregation Pipeline: Viajes por d√≠a de la semana
if 'collection' in locals() and collection.count_documents({}) > 0:
    pipeline_by_weekday = [
        {
            "$group": {
                "_id": "$pickup_day_of_week",
                "day_name": {"$first": "$pickup_day_name"},
                "total_trips": {"$sum": 1},
                "avg_duration": {"$avg": "$trip_duration_minutes"}
            }
        },
        {
            "$sort": {"_id": 1}
        }
    ]
    
    logger.info("üîç Ejecutando consulta: Viajes por d√≠a de la semana")
    results = list(collection.aggregate(pipeline_by_weekday))
    
    df_by_weekday = pd.DataFrame(results)
    
    # Mapeo de d√≠as en espa√±ol
    day_map = {
        'Monday': 'Lunes', 'Tuesday': 'Martes', 'Wednesday': 'Mi√©rcoles',
        'Thursday': 'Jueves', 'Friday': 'Viernes', 'Saturday': 'S√°bado', 'Sunday': 'Domingo'
    }
    
    if 'day_name' in df_by_weekday.columns:
        df_by_weekday['Dia'] = df_by_weekday['day_name'].map(day_map)
    else:
        dias_semana = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']
        df_by_weekday['Dia'] = [dias_semana[i] for i in df_by_weekday['_id']]
    
    df_by_weekday = df_by_weekday[['Dia', 'total_trips', 'avg_duration']]
    df_by_weekday.columns = ['Dia', 'Total_Viajes', 'Duracion_Promedio_Min']
    
    print("\nüìÖ VIAJES POR D√çA DE LA SEMANA")
    print("="*80)
    display(df_by_weekday)
else:
    print("‚ö†Ô∏è  No hay datos para consultar")


# Visualizaci√≥n: Viajes por d√≠a de la semana
if 'df_by_weekday' in locals():
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = ['#FF6B6B' if dia in ['S√°bado', 'Domingo'] else '#4ECDC4' 
              for dia in df_by_weekday['Dia']]
    
    bars = ax.bar(df_by_weekday['Dia'], df_by_weekday['Total_Viajes'],
                   color=colors, edgecolor='black', alpha=0.8, linewidth=1.5)
    
    ax.set_xlabel('D√≠a de la Semana', fontsize=12, fontweight='bold')
    ax.set_ylabel('N√∫mero de Viajes', fontsize=12, fontweight='bold')
    ax.set_title('üìÖ Distribuci√≥n de Viajes por D√≠a de la Semana', fontsize=14, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.ticklabel_format(style='plain', axis='y')
    
    # Rotar etiquetas
    plt.xticks(rotation=45, ha='right')
    
    # A√±adir leyenda
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#4ECDC4', edgecolor='black', label='D√≠a laboral'),
        Patch(facecolor='#FF6B6B', edgecolor='black', label='Fin de semana')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    
    output_path = Path('./outputs/trips_by_weekday.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"üíæ Gr√°fico guardado en: {output_path}")
    
    plt.show()





# Aggregation Pipeline: Estad√≠sticas generales
if 'collection' in locals() and collection.count_documents({}) > 0:
    pipeline_stats = [
        {
            "$group": {
                "_id": None,
                "total_trips": {"$sum": 1},
                "avg_duration_min": {"$avg": "$trip_duration_minutes"},
                "min_duration_min": {"$min": "$trip_duration_minutes"},
                "max_duration_min": {"$max": "$trip_duration_minutes"},
                "avg_distance_miles": {"$avg": "$trip_miles"},
                "total_distance_miles": {"$sum": "$trip_miles"}
            }
        }
    ]
    
    logger.info("üîç Ejecutando consulta: Estad√≠sticas generales")
    results = list(collection.aggregate(pipeline_stats))
    
    if results:
        stats = results[0]
        
        print("\nüìä ESTAD√çSTICAS GENERALES DEL DATASET")
        print("="*80)
        print(f"üöï Total de viajes analizados: {stats.get('total_trips', 0):,}")
        print(f"\n‚è±Ô∏è  DURACI√ìN DE VIAJES:")
        print(f"   ‚Ä¢ Promedio: {stats.get('avg_duration_min', 0):.2f} minutos")
        print(f"   ‚Ä¢ M√≠nima: {stats.get('min_duration_min', 0):.2f} minutos")
        print(f"   ‚Ä¢ M√°xima: {stats.get('max_duration_min', 0):.2f} minutos")
        
        if stats.get('avg_distance_miles'):
            print(f"\nüìè DISTANCIA RECORRIDA:")
            print(f"   ‚Ä¢ Promedio por viaje: {stats.get('avg_distance_miles', 0):.2f} millas")
            print(f"   ‚Ä¢ Total acumulada: {stats.get('total_distance_miles', 0):,.2f} millas")
            print(f"   ‚Ä¢ Equivalente en km: {stats.get('total_distance_miles', 0) * 1.60934:,.2f} km")
else:
    print("‚ö†Ô∏è  No hay datos para consultar")





# Aggregation Pipeline: Distribuci√≥n de duraciones en bins
if 'collection' in locals() and collection.count_documents({}) > 0:
    pipeline_duration_dist = [
        {
            "$bucket": {
                "groupBy": "$trip_duration_minutes",
                "boundaries": [0, 5, 10, 15, 20, 30, 45, 60, 90, 120, 240],
                "default": "240+",
                "output": {
                    "count": {"$sum": 1},
                    "avg_distance": {"$avg": "$trip_miles"}
                }
            }
        }
    ]
    
    logger.info("üîç Ejecutando consulta: Distribuci√≥n de duraciones")
    results = list(collection.aggregate(pipeline_duration_dist))
    
    df_duration_dist = pd.DataFrame(results)
    df_duration_dist.columns = ['Rango_Minutos', 'Cantidad_Viajes', 'Distancia_Promedio']
    
    # Crear etiquetas legibles
    labels = ['0-5', '5-10', '10-15', '15-20', '20-30', '30-45', '45-60', '60-90', '90-120', '120-240', '240+']
    df_duration_dist['Etiqueta'] = labels[:len(df_duration_dist)]
    
    print("\nüìà DISTRIBUCI√ìN DE DURACIONES DE VIAJE")
    print("="*80)
    display(df_duration_dist[['Etiqueta', 'Cantidad_Viajes', 'Distancia_Promedio']])
else:
    print("‚ö†Ô∏è  No hay datos para consultar")


# Visualizaci√≥n: Histograma de duraciones
if 'df_duration_dist' in locals():
    fig, ax = plt.subplots(figsize=(14, 6))
    
    bars = ax.bar(df_duration_dist['Etiqueta'], df_duration_dist['Cantidad_Viajes'],
                   color='skyblue', edgecolor='navy', alpha=0.7, linewidth=1.5)
    
    # Colorear la barra m√°s alta
    max_idx = df_duration_dist['Cantidad_Viajes'].idxmax()
    bars[max_idx].set_color('coral')
    bars[max_idx].set_edgecolor('darkred')
    
    ax.set_xlabel('Duraci√≥n del Viaje (minutos)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Cantidad de Viajes', fontsize=12, fontweight='bold')
    ax.set_title('üìà Distribuci√≥n de Duraciones de Viajes', fontsize=14, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.ticklabel_format(style='plain', axis='y')
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    output_path = Path('./outputs/duration_distribution.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"üíæ Gr√°fico guardado en: {output_path}")
    
    plt.show()





# Aggregation Pipeline: An√°lisis por plataforma (Uber, Lyft, etc.)
if 'collection' in locals() and collection.count_documents({}) > 0:
    pipeline_by_platform = [
        {
            "$group": {
                "_id": "$hvfhs_license_num",
                "total_trips": {"$sum": 1},
                "avg_duration": {"$avg": "$trip_duration_minutes"},
                "avg_distance": {"$avg": "$trip_miles"}
            }
        },
        {
            "$sort": {"total_trips": -1}
        }
    ]
    
    logger.info("üîç Ejecutando consulta: Viajes por plataforma")
    results = list(collection.aggregate(pipeline_by_platform))
    
    if results:
        df_by_platform = pd.DataFrame(results)
        df_by_platform.columns = ['Plataforma', 'Total_Viajes', 'Duracion_Promedio', 'Distancia_Promedio']
        
        # Mapeo de c√≥digos a nombres de plataformas (c√≥digos comunes)
        platform_names = {
            'HV0003': 'Uber',
            'HV0005': 'Lyft',
            'HV0004': 'Via'
        }
        
        df_by_platform['Nombre'] = df_by_platform['Plataforma'].map(platform_names).fillna(df_by_platform['Plataforma'])
        
        print("\nüöñ AN√ÅLISIS POR PLATAFORMA")
        print("="*80)
        display(df_by_platform[['Nombre', 'Total_Viajes', 'Duracion_Promedio', 'Distancia_Promedio']])
    else:
        print("‚ö†Ô∏è  No se encontraron datos de plataformas")
else:
    print("‚ö†Ô∏è  No hay datos para consultar")


# Visualizaci√≥n: Market share por plataforma
if 'df_by_platform' in locals():
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Gr√°fico 1: Pie chart de market share
    colors = sns.color_palette('Set2', len(df_by_platform))
    wedges, texts, autotexts = ax1.pie(
        df_by_platform['Total_Viajes'],
        labels=df_by_platform['Nombre'],
        autopct='%1.1f%%',
        startangle=90,
        colors=colors,
        explode=[0.05] * len(df_by_platform)
    )
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(10)
    
    ax1.set_title('üöñ Market Share por Plataforma', fontsize=14, fontweight='bold')
    
    # Gr√°fico 2: Comparaci√≥n de m√©tricas
    x = np.arange(len(df_by_platform))
    width = 0.35
    
    ax2_twin = ax2.twinx()
    
    bars1 = ax2.bar(x - width/2, df_by_platform['Duracion_Promedio'],
                     width, label='Duraci√≥n Promedio (min)', color='skyblue', edgecolor='black')
    bars2 = ax2_twin.bar(x + width/2, df_by_platform['Distancia_Promedio'],
                          width, label='Distancia Promedio (mi)', color='coral', edgecolor='black')
    
    ax2.set_xlabel('Plataforma', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Duraci√≥n Promedio (min)', fontsize=11, fontweight='bold', color='skyblue')
    ax2_twin.set_ylabel('Distancia Promedio (mi)', fontsize=11, fontweight='bold', color='coral')
    ax2.set_title('üìä Comparaci√≥n de M√©tricas por Plataforma', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(df_by_platform['Nombre'], rotation=45, ha='right')
    ax2.grid(axis='y', alpha=0.3)
    
    # Leyendas
    lines1, labels1 = ax2.get_legend_handles_labels()
    lines2, labels2 = ax2_twin.get_legend_handles_labels()
    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
    
    plt.tight_layout()
    
    output_path = Path('./outputs/platform_analysis.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"üíæ Gr√°fico guardado en: {output_path}")
    
    plt.show()





# Crear dashboard con m√©tricas clave
if 'collection' in locals() and collection.count_documents({}) > 0:
    # Recopilar m√©tricas
    total_trips = collection.count_documents({})
    
    # Calcular promedios
    pipeline_summary = [
        {
            "$group": {
                "_id": None,
                "avg_duration": {"$avg": "$trip_duration_minutes"},
                "avg_distance": {"$avg": "$trip_miles"},
                "total_distance": {"$sum": "$trip_miles"}
            }
        }
    ]
    
    summary = list(collection.aggregate(pipeline_summary))[0]
    
    # Crear figura del dashboard
    fig = plt.figure(figsize=(16, 10))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # Color scheme profesional
    colors_palette = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E']
    
    # M√©trica 1: Total de viajes
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.text(0.5, 0.6, f"{total_trips:,}", ha='center', va='center', 
             fontsize=36, fontweight='bold', color=colors_palette[0])
    ax1.text(0.5, 0.3, "Viajes Totales", ha='center', va='center',
             fontsize=14, color='gray')
    ax1.axis('off')
    ax1.set_facecolor('#f0f0f0')
    
    # M√©trica 2: Duraci√≥n promedio
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.text(0.5, 0.6, f"{summary['avg_duration']:.1f}", ha='center', va='center',
             fontsize=36, fontweight='bold', color=colors_palette[1])
    ax2.text(0.5, 0.3, "Duraci√≥n Promedio (min)", ha='center', va='center',
             fontsize=14, color='gray')
    ax2.axis('off')
    ax2.set_facecolor('#f0f0f0')
    
    # M√©trica 3: Distancia promedio
    ax3 = fig.add_subplot(gs[0, 2])
    if summary.get('avg_distance'):
        ax3.text(0.5, 0.6, f"{summary['avg_distance']:.2f}", ha='center', va='center',
                 fontsize=36, fontweight='bold', color=colors_palette[2])
        ax3.text(0.5, 0.3, "Distancia Promedio (mi)", ha='center', va='center',
                 fontsize=14, color='gray')
    else:
        ax3.text(0.5, 0.5, "N/A", ha='center', va='center',
                 fontsize=36, fontweight='bold', color='gray')
    ax3.axis('off')
    ax3.set_facecolor('#f0f0f0')
    
    # Gr√°fico 4: Mini viajes por hora (si existe el dato)
    if 'df_trips_by_hour' in locals():
        ax4 = fig.add_subplot(gs[1, :])
        ax4.fill_between(df_trips_by_hour['Hora'], df_trips_by_hour['Total_Viajes'],
                          alpha=0.6, color=colors_palette[0])
        ax4.plot(df_trips_by_hour['Hora'], df_trips_by_hour['Total_Viajes'],
                 marker='o', linewidth=2, color=colors_palette[0])
        ax4.set_title('Tendencia de Viajes por Hora', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Hora del D√≠a')
        ax4.set_ylabel('N√∫mero de Viajes')
        ax4.grid(alpha=0.3)
        ax4.set_xticks(range(0, 24, 2))
    
    # Gr√°fico 5: Top 5 zonas (si existe el dato)
    if 'df_top_pickup' in locals():
        ax5 = fig.add_subplot(gs[2, :2])
        top_5 = df_top_pickup.head(5)
        ax5.barh(top_5['LocationID'].astype(str), top_5['Total_Recogidas'],
                 color=colors_palette[3], edgecolor='black', alpha=0.7)
        ax5.set_title('Top 5 Zonas de Recogida', fontsize=14, fontweight='bold')
        ax5.set_xlabel('N√∫mero de Recogidas')
        ax5.invert_yaxis()
        ax5.grid(axis='x', alpha=0.3)
    
    # Gr√°fico 6: Distribuci√≥n d√≠a de semana (si existe)
    if 'df_by_weekday' in locals():
        ax6 = fig.add_subplot(gs[2, 2])
        ax6.pie(df_by_weekday['Total_Viajes'], labels=df_by_weekday['Dia'],
                autopct='%1.0f%%', startangle=90, colors=sns.color_palette('pastel'))
        ax6.set_title('Distribuci√≥n Semanal', fontsize=12, fontweight='bold')
    
    plt.suptitle('üöï NYC HVFHV - Dashboard de M√©tricas Clave', 
                 fontsize=18, fontweight='bold', y=0.98)
    
    output_path = Path('./outputs/dashboard_metricas.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    logger.info(f"üíæ Dashboard guardado en: {output_path}")
    
    plt.show()
else:
    print("‚ö†Ô∏è  No hay datos para generar el dashboard")











# Funci√≥n para exportar resultados de consulta a CSV
def export_query_to_csv(pipeline, filename, collection):
    """
    Ejecuta un aggregation pipeline y exporta resultados a CSV.
    
    Args:
        pipeline (list): Aggregation pipeline de MongoDB
        filename (str): Nombre del archivo CSV de salida
        collection: Colecci√≥n de MongoDB
    """
    try:
        results = list(collection.aggregate(pipeline))
        df = pd.DataFrame(results)
        
        output_path = Path(f'./outputs/{filename}')
        df.to_csv(output_path, index=False, encoding='utf-8')
        
        logger.info(f"‚úÖ Resultados exportados a: {output_path}")
        print(f"üìÑ Archivo guardado: {output_path}")
        return df
    
    except Exception as e:
        logger.error(f"‚ùå Error al exportar: {str(e)}")
        return None

print("‚úÖ Funci√≥n export_query_to_csv() disponible")


# Funci√≥n para obtener estad√≠sticas de la colecci√≥n
def get_collection_stats(collection):
    """
    Obtiene estad√≠sticas de la colecci√≥n de MongoDB.
    
    Args:
        collection: Colecci√≥n de MongoDB
    """
    try:
        stats = collection.database.command("collStats", collection.name)
        
        print("\nüìä ESTAD√çSTICAS DE LA COLECCI√ìN")
        print("="*80)
        print(f"üì¶ Nombre: {stats['ns']}")
        print(f"üìÑ Documentos: {stats['count']:,}")
        print(f"üíæ Tama√±o de datos: {stats['size'] / (1024**2):.2f} MB")
        print(f"üíø Tama√±o en disco: {stats.get('storageSize', 0) / (1024**2):.2f} MB")
        print(f"üîç √çndices: {stats['nindexes']}")
        print(f"üìä Tama√±o de √≠ndices: {stats.get('totalIndexSize', 0) / (1024**2):.2f} MB")
        
        return stats
    
    except Exception as e:
        logger.error(f"‚ùå Error al obtener estad√≠sticas: {str(e)}")
        return None

# Obtener estad√≠sticas si hay conexi√≥n
if 'collection' in locals():
    get_collection_stats(collection)


# ADVERTENCIA: Esta celda elimina TODOS los datos de la colecci√≥n
# Descomenta solo si deseas limpiar completamente la base de datos

def clear_collection(collection):
    """
    Elimina todos los documentos de la colecci√≥n.
    
    Args:
        collection: Colecci√≥n de MongoDB
    """
    try:
        result = collection.delete_many({})
        logger.info(f"üóëÔ∏è  Eliminados {result.deleted_count:,} documentos")
        print(f"‚úÖ Colecci√≥n limpiada: {result.deleted_count:,} documentos eliminados")
    except Exception as e:
        logger.error(f"‚ùå Error al limpiar colecci√≥n: {str(e)}")

# Descomenta para ejecutar
# if 'collection' in locals():
#     clear_collection(collection)

print("üí° Funci√≥n clear_collection() disponible (descomenta para usar)")








# Cerrar conexi√≥n a MongoDB (buena pr√°ctica)
if 'mongo_client' in locals():
    mongo_client.close()
    logger.info("üîå Conexi√≥n a MongoDB cerrada")
    print("‚úÖ Conexi√≥n cerrada correctamente")

logger.info("=" * 80)
logger.info("FIN DEL AN√ÅLISIS - Proyecto completado exitosamente")
logger.info("=" * 80)


# OPCIONAL: Funci√≥n para exportar Parquet a JSON
def parquet_to_json(parquet_file, output_json_file, sample_rows=None):
    """
    Convierte un archivo Parquet a JSON.
    
    Args:
        parquet_file (Path): Ruta al archivo Parquet
        output_json_file (Path): Ruta de salida para el JSON
        sample_rows (int, optional): N√∫mero de filas a exportar (None = todas)
    
    Returns:
        int: N√∫mero de registros exportados
    """
    try:
        logger.info(f"üîÑ Convirtiendo {parquet_file.name} a JSON...")
        
        # Leer Parquet
        df = pd.read_parquet(parquet_file, engine='pyarrow')
        
        if sample_rows:
            df = df.head(sample_rows)
            logger.info(f"   üìä Exportando muestra de {sample_rows:,} filas")
        
        # Limpiar datos (usar funci√≥n existente)
        df_clean = clean_hvfhv_data(df)
        
        # Convertir datetime a string para JSON
        for col in df_clean.select_dtypes(include=['datetime64']).columns:
            df_clean[col] = df_clean[col].astype(str)
        
        # Exportar a JSON
        df_clean.to_json(output_json_file, orient='records', lines=True, date_format='iso')
        
        file_size_mb = output_json_file.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ JSON exportado: {output_json_file.name} ({file_size_mb:.2f} MB)")
        
        return len(df_clean)
    
    except Exception as e:
        logger.error(f"‚ùå Error al convertir a JSON: {str(e)}")
        raise

# Ejemplo de uso (descomenta para ejecutar):
# if parquet_files:
#     sample_file = parquet_files[0]
#     output_json = Path('./data/sample_output.json')
#     records_exported = parquet_to_json(sample_file, output_json, sample_rows=10000)
#     print(f"‚úÖ Exportados {records_exported:,} registros a JSON")

print("üí° Funci√≥n parquet_to_json() disponible (NO NECESARIA para MongoDB)")
